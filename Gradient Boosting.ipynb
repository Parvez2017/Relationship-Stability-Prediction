{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# For reading, visualizing, and preprocessing data\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.feature_selection import RFE, RFECV\nfrom sklearn.model_selection import train_test_split, GridSearchCV, KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\n\n# Classifiers\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.linear_model import RidgeClassifier, SGDClassifier\nfrom sklearn.naive_bayes import BernoulliNB, GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import LinearSVC, NuSVC, SVC\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import accuracy_score \nfrom sklearn.metrics import f1_score\nfrom imblearn.metrics import geometric_mean_score\nfrom imblearn.metrics import sensitivity_score\nfrom imblearn.metrics import specificity_score\nfrom sklearn.metrics import roc_auc_score\n\n\n###############################################################################\n#                                 2. Get data                                 #\n###############################################################################\ndf = pd.read_csv(\"../input/promisin-couples/promising_couples.csv\")\n#df.replace(to_replace = -1 , value =np.nan)\n\nX = df.iloc[:, 0:139].values\ny = df.iloc[:, 139].values\n\n#imputing missing values\nfrom sklearn.impute import KNNImputer\nimputer = KNNImputer()\n#imputer = Imputer(missing_values = 'NaN', strategy = 'most_frequent', axis = 0)\nimputer.fit(X[:, 0:139])\nX[:, 0:139] = imputer.transform(X[:, 0:139])\n\n#Making all the values discrete\nfrom sklearn.preprocessing import KBinsDiscretizer\nest = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\nX = est.fit_transform(X)\n\n# Filter Method: Spearman's Cross Correlation > 0.95\n# Make correlation matrix\ncorr_matrix = pd.DataFrame(X).corr(method = \"spearman\").abs()\n\n# Draw the heatmap\nsns.set(font_scale = 1.0)\nf, ax = plt.subplots(figsize=(11, 9))\nsns.heatmap(corr_matrix, cmap= \"YlGnBu\", square=True, ax = ax)\nf.tight_layout()\nplt.savefig(\"correlation_matrix.png\", dpi = 1080)\n\n# Select upper triangle of matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n\n# Drop features\nX =pd.DataFrame(X).drop(to_drop, axis = 1)\n\n###############################################################################\n#                  8. Custom pipeline object to use with RFECV                #\n###############################################################################\n# Select Features using RFECV\nclass PipelineRFE(Pipeline):\n    # Source: https://ramhiser.com/post/2018-03-25-feature-selection-with-scikit-learn-pipeline/\n    def fit(self, X, y=None, **fit_params):\n        super(PipelineRFE, self).fit(X, y, **fit_params)\n        self.feature_importances_ = self.steps[-1][-1].feature_importances_\n        return self\n\nscaler = StandardScaler()\nestimator = RandomForestClassifier(n_estimators= 200,\n                                   class_weight ='balanced',\n                                   max_features = 'auto',\n                                   max_depth = 6,\n                                   min_samples_split = 0.005,\n                                   min_samples_leaf = 0.005,\n                                   criterion = 'entropy',\n                                   n_jobs = -1)\nsteps = [(\"scaler\", scaler), (\"classifier\", estimator)]\npipe = PipelineRFE(steps = steps)\n\n# Initialize RFECV object\nfeature_selector = RFECV(pipe, cv = 5, step = 1, min_features_to_select=10, scoring = \"roc_auc\", verbose = 1)\n\n# Fit RFECV\nX = feature_selector.fit_transform(X, y)\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20,\n                                                    random_state = 1000)\n\nscaler = StandardScaler()\n \nclassifier = GradientBoostingClassifier()\n# Define steps in pipeline\nsteps = [(\"scaler\", scaler), (\"classifier\", classifier)]\n    \n# Initialize Pipeline object \npipeline = Pipeline(steps = steps)\n      \n# Define parameter grid\nparam_grid = { \"classifier__learning_rate\":[0.15,0.1,0.05,0.01,0.005,0.001], \n                                        \"classifier__n_estimators\": [200],\n                                        \"classifier__max_depth\": [2,3,4,5,6],\n                                        \"classifier__min_samples_split\": [0.005, 0.01, 0.05, 0.10],\n                                        \"classifier__min_samples_leaf\": [0.005, 0.01, 0.05, 0.10],\n                                        \"classifier__max_features\": [\"auto\", \"sqrt\", \"log2\"],\n                                        \"classifier__subsample\": [0.8, 0.9, 1]}\n    \n# Initialize GridSearch object\ngscv = GridSearchCV(pipeline, param_grid, cv = 10,  n_jobs= -1, verbose = 1, scoring = \"roc_auc\")\n                      \n# Fit gscv\ngscv.fit(X_train, y_train) \n    \nclf = gscv.best_estimator_\n    \n    \ny_pred = clf.predict(X_test)\n    \n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Accuracy :       ', accuracy_score(y_test, y_pred))\nprint('ROC :            ', roc_auc_score(y_test, y_pred))\nprint('F-Measure :      ', f1_score(y_test, y_pred, average = 'weighted'))\nprint('Geometric Mean : ', geometric_mean_score(y_test, y_pred, average = 'weighted'))\nprint('Sensitivity :    ', sensitivity_score(y_test, y_pred, average = 'weighted'))\nprint('Specificity :    ', specificity_score(y_test, y_pred, average = 'weighted'))\nprint('Type I Error :   ', (1-geometric_mean_score(y_test, y_pred, average = 'weighted')))\nprint('Type II Error :  ', (1-specificity_score(y_test, y_pred, average = 'weighted')))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pickle\nfilename = 'Gradient_boosting.sav'\npickle.dump(clf, open(filename, 'wb'))\n \n# some time later...\n \n# load the model from disk\nloaded_model = pickle.load(open(filename, 'rb'))\nresult = loaded_model.score(X_test, y_test)\nprint(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gscv.best_estimator_","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}