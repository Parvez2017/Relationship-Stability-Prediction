{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.feature_selection import RFE, RFECV\nfrom sklearn.model_selection import train_test_split, GridSearchCV, KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\n\n# Classifiers\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.linear_model import RidgeClassifier, SGDClassifier\nfrom sklearn.naive_bayes import BernoulliNB, GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import LinearSVC, NuSVC, SVC\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############################################################################\n#                                 2. Get data                                 #\n###############################################################################\ndf = pd.read_csv(\"../input/promising-couple/wave12345.csv\")\n#df.replace(to_replace = -1 , value =np.nan)\n\nX = df.iloc[:, 2:46].values\ny = df.iloc[:, 46].values\n\n#imputing missing values\nfrom sklearn.impute import KNNImputer\nimputer = KNNImputer()\n#imputer = Imputer(missing_values = 'NaN', strategy = 'most_frequent', axis = 0)\nimputer.fit(X[:, 0:44])\nX[:, 0:44] = imputer.transform(X[:, 0:44])\n\n#Making all the values discrete\nfrom sklearn.preprocessing import KBinsDiscretizer\nest = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\nX = est.fit_transform(X)\n\n###############################################################################\n#                        3. Create train and test set                         #\n###############################################################################\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20,\n                                                    random_state = 1000)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"classifiers = {}\nclassifiers.update({\"Random Forest\": RandomForestClassifier()})\nclassifiers.update({\"Ridge\": RidgeClassifier()})\nclassifiers.update({\"SGD\": SGDClassifier()})\nclassifiers.update({\"BNB\": BernoulliNB()})\nclassifiers.update({\"GNB\": GaussianNB()})\nclassifiers.update({\"KNN\": KNeighborsClassifier()})\n\n# Create dict of decision function labels\nDECISION_FUNCTIONS = {\"Ridge\", \"SGD\"}\n\n# Create dict for classifiers with feature_importances_ attribute\nFEATURE_IMPORTANCE = {\"Random Forest\"}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initiate parameter grid\nparameters = {}\n\n# Update dict with Random Forest Parameters\nparameters.update({\"Random Forest\": { \n                                    \"classifier__n_estimators\": [200],\n                                    \"classifier__class_weight\": [None, \"balanced\"],\n                                    \"classifier__max_features\": [\"auto\", \"sqrt\", \"log2\"],\n                                    \"classifier__max_depth\" : [3, 4, 5, 6, 7, 8],\n                                    \"classifier__min_samples_split\": [0.005, 0.01, 0.05, 0.10],\n                                    \"classifier__min_samples_leaf\": [0.005, 0.01, 0.05, 0.10],\n                                    \"classifier__criterion\" :[\"gini\", \"entropy\"]     ,\n                                    \"classifier__n_jobs\": [-1]\n                                     }})\n\n# Update dict with Ridge\nparameters.update({\"Ridge\": { \n                            \"classifier__alpha\": [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.25, 0.50, 0.75, 1.0]\n                             }})\n\n# Update dict with SGD Classifier\nparameters.update({\"SGD\": { \n                            \"classifier__alpha\": [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.25, 0.50, 0.75, 1.0],\n                            \"classifier__penalty\": [\"l1\", \"l2\"],\n                            \"classifier__n_jobs\": [-1]\n                             }})\n\n\n# Update dict with BernoulliNB Classifier\nparameters.update({\"BNB\": { \n                            \"classifier__alpha\": [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.25, 0.50, 0.75, 1.0]\n                             }})\n\n# Update dict with GaussianNB Classifier\nparameters.update({\"GNB\": { \n                            \"classifier__var_smoothing\": [1e-9, 1e-8,1e-7, 1e-6, 1e-5]\n                             }})\n\n# Update dict with K Nearest Neighbors Classifier\nparameters.update({\"KNN\": { \n                            \"classifier__n_neighbors\": list(range(1,31)),\n                            \"classifier__p\": [1, 2, 3, 4, 5],\n                            \"classifier__leaf_size\": [5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n                            \"classifier__n_jobs\": [-1]\n                             }})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############################################################################\n#              6. Feature Selection: Removing highly correlated features      #\n###############################################################################\n# Filter Method: Spearman's Cross Correlation > 0.95\n# Make correlation matrix\ncorr_matrix = pd.DataFrame(X_train).corr(method = \"spearman\").abs()\n\n# Draw the heatmap\nsns.set(font_scale = 1.0)\nf, ax = plt.subplots(figsize=(11, 9))\nsns.heatmap(corr_matrix, cmap= \"YlGnBu\", square=True, ax = ax)\nf.tight_layout()\nplt.savefig(\"correlation_matrix.png\", dpi = 1080)\n\n# Select upper triangle of matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n\n# Drop features\nX_train =pd.DataFrame(X_train).drop(to_drop, axis = 1)\nX_test = pd.DataFrame(X_test).drop(to_drop, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############################################################################\n#                     7. Tuning a classifier to use with RFECV                #\n###############################################################################\n# Define classifier to use as the base of the recursive feature elimination algorithm\nselected_classifier = \"Random Forest\"\nclassifier = classifiers[selected_classifier]\n\n# Tune classifier (Took = 4.8 minutes)\n    \n# Scale features via Z-score normalization\nscaler = StandardScaler()\n\n# Define steps in pipeline\nsteps = [(\"scaler\", scaler), (\"classifier\", classifier)]\n\n# Initialize Pipeline object\npipeline = Pipeline(steps = steps)\n  \n# Define parameter grid\nparam_grid = parameters[selected_classifier]\n\n# Initialize GridSearch object\ngscv = GridSearchCV(pipeline, param_grid, cv = 5,  n_jobs= -1, verbose = 1, scoring = \"roc_auc\")\n                  \n# Fit gscv\nprint(f\"Now tuning {selected_classifier}. Go grab a beer or something.\")\ngscv.fit(X_train, np.ravel(y_train))  \n\n# Get best parameters and score\nbest_params = gscv.best_params_\nbest_score = gscv.best_score_\n        \n# Update classifier parameters\ntuned_params = {item[12:]: best_params[item] for item in best_params}\nclassifier.set_params(**tuned_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############################################################################\n#                  8. Custom pipeline object to use with RFECV                #\n###############################################################################\n# Select Features using RFECV\nclass PipelineRFE(Pipeline):\n    # Source: https://ramhiser.com/post/2018-03-25-feature-selection-with-scikit-learn-pipeline/\n    def fit(self, X, y=None, **fit_params):\n        super(PipelineRFE, self).fit(X, y, **fit_params)\n        self.feature_importances_ = self.steps[-1][-1].feature_importances_\n        return self\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############################################################################\n#   9. Feature Selection: Recursive Feature Selection with Cross Validation   #\n###############################################################################\n# Define pipeline for RFECV\nsteps = [(\"scaler\", scaler), (\"classifier\", classifier)]\npipe = PipelineRFE(steps = steps)\n\n# Initialize RFECV object\nfeature_selector = RFECV(pipe, cv = 5, step = 1, scoring = \"roc_auc\", verbose = 1)\n\n# Fit RFECV\nfeature_selector.fit(X_train, np.ravel(y_train))\n\n# Get selected features\nfeature_names = X_train.columns\nselected_features = feature_names[feature_selector.support_].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############################################################################\n#                             10. Performance Curve                           #\n###############################################################################\n# Get Performance Data\nperformance_curve = {\"Number of Features\": list(range(1, len(feature_names) + 1)),\n                    \"AUC\": feature_selector.grid_scores_}\nperformance_curve = pd.DataFrame(performance_curve)\n\n# Performance vs Number of Features\n# Set graph style\nsns.set(font_scale = 1.75)\nsns.set_style({\"axes.facecolor\": \"1.0\", \"axes.edgecolor\": \"0.85\", \"grid.color\": \"0.85\",\n               \"grid.linestyle\": \"-\", 'axes.labelcolor': '0.4', \"xtick.color\": \"0.4\",\n               'ytick.color': '0.4'})\ncolors = sns.color_palette(\"RdYlGn\", 20)\nline_color = colors[3]\nmarker_colors = colors[-1]\n\n# Plot\nf, ax = plt.subplots(figsize=(13, 6.5))\nsns.lineplot(x = \"Number of Features\", y = \"AUC\", data = performance_curve,\n             color = line_color, lw = 4, ax = ax)\nsns.regplot(x = performance_curve[\"Number of Features\"], y = performance_curve[\"AUC\"],\n            color = marker_colors, fit_reg = False, scatter_kws = {\"s\": 200}, ax = ax)\n\n# Axes limits\nplt.xlim(0.5, len(feature_names)+0.5)\nplt.ylim(0.60, 0.925)\n\n# Generate a bolded horizontal line at y = 0\nax.axhline(y = 0.625, color = 'black', linewidth = 1.3, alpha = .7)\n\n# Turn frame off\nax.set_frame_on(False)\n\n# Tight layout\nplt.tight_layout()\n\n# Save Figure\nplt.savefig(\"performance_curve.png\", dpi = 1080)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############################################################################\n#                11. Feature Selection: Recursive Feature Selection           #\n###############################################################################\n# Define pipeline for RFECV\nsteps = [(\"scaler\", scaler), (\"classifier\", classifier)]\npipe = PipelineRFE(steps = steps)\n\n# Initialize RFE object\nfeature_selector = RFE(pipe, n_features_to_select = 10, step = 1, verbose = 1)\n\n# Fit RFE\nfeature_selector.fit(X_train, np.ravel(y_train))\n\n# Get selected features labels\nfeature_names = X_train.columns\nselected_features = feature_names[feature_selector.support_].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############################################################################\n#                  12. Visualizing Selected Features Importance               #\n###############################################################################\n# Get selected features data set\nX_train = X_train[selected_features]\nX_test = X_test[selected_features]\n\n# Train classifier\nclassifier.fit(X_train, np.ravel(y_train))\n\n# Get feature importance\nfeature_importance = pd.DataFrame(selected_features, columns = [\"Feature Label\"])\nfeature_importance[\"Feature Importance\"] = classifier.feature_importances_\n\n# Sort by feature importance\nfeature_importance = feature_importance.sort_values(by=\"Feature Importance\", ascending=False)\n\n# Set graph style\nsns.set(font_scale = 1.75)\nsns.set_style({\"axes.facecolor\": \"1.0\", \"axes.edgecolor\": \"0.85\", \"grid.color\": \"0.85\",\n               \"grid.linestyle\": \"-\", 'axes.labelcolor': '0.4', \"xtick.color\": \"0.4\",\n               'ytick.color': '0.4'})\n\n# Set figure size and create barplot\nf, ax = plt.subplots(figsize=(12, 9))\nsns.barplot(x = \"Feature Importance\", y = \"Feature Label\",\n            palette = reversed(sns.color_palette('YlOrRd', 15)),  data = feature_importance)\n\n# Generate a bolded horizontal line at y = 0\nax.axvline(x = 0, color = 'black', linewidth = 4, alpha = .7)\n\n# Turn frame off\nax.set_frame_on(False)\n\n# Tight layout\nplt.tight_layout()\n\n# Save Figure\nplt.savefig(\"feature_importance.png\", dpi = 1080)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############################################################################\n#                       13. Classifier Tuning and Evaluation                  #\n###############################################################################\n# Initialize dictionary to store results\nresults = {}\n\n# Tune and evaluate classifiers\nfor classifier_label, classifier in classifiers.items():\n    # Print message to user\n    print(f\"Now tuning {classifier_label}.\")\n    \n    # Scale features via Z-score normalization\n    scaler = StandardScaler()\n    \n    # Define steps in pipeline\n    steps = [(\"scaler\", scaler), (\"classifier\", classifier)]\n    \n    # Initialize Pipeline object \n    pipeline = Pipeline(steps = steps)\n      \n    # Define parameter grid\n    param_grid = parameters[classifier_label]\n    \n    # Initialize GridSearch object\n    gscv = GridSearchCV(pipeline, param_grid, cv = 5,  n_jobs= -1, verbose = 1, scoring = \"roc_auc\")\n                      \n    # Fit gscv\n    gscv.fit(X_train, np.ravel(y_train))  \n    \n    # Get best parameters and score\n    best_params = gscv.best_params_\n    best_score = gscv.best_score_\n    \n    # Update classifier parameters and define new pipeline with tuned classifier\n    tuned_params = {item[12:]: best_params[item] for item in best_params}\n    classifier.set_params(**tuned_params)\n            \n    # Make predictions\n    if classifier_label in DECISION_FUNCTIONS:\n        y_pred = gscv.decision_function(X_test)\n    else:\n        y_pred = gscv.predict_proba(X_test)[:,1]\n      \n    # Evaluate model\n    auc = metrics.roc_auc_score(y_test, y_pred)\n    \n    # Save results\n    result = {\"Classifier\": gscv,\n              \"Best Parameters\": best_params,\n              \"Training AUC\": best_score,\n              \"Test AUC\": auc}\n    \n    results.update({classifier_label: result})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############################################################################\n#                              14. Visualing Results                          #\n###############################################################################\n# Initialize auc_score dictionary\nauc_scores = {\n              \"Classifier\": [],\n              \"AUC\": [],\n              \"AUC Type\": []\n              }\n\n# Get AUC scores into dictionary\nfor classifier_label in results:\n    auc_scores.update({\"Classifier\": [classifier_label] + auc_scores[\"Classifier\"],\n                       \"AUC\": [results[classifier_label][\"Training AUC\"]] + auc_scores[\"AUC\"],\n                       \"AUC Type\": [\"Training\"] + auc_scores[\"AUC Type\"]})\n    \n    auc_scores.update({\"Classifier\": [classifier_label] + auc_scores[\"Classifier\"],\n                       \"AUC\": [results[classifier_label][\"Test AUC\"]] + auc_scores[\"AUC\"],\n                       \"AUC Type\": [\"Test\"] + auc_scores[\"AUC Type\"]})\n\n# Dictionary to PandasDataFrame\nauc_scores = pd.DataFrame(auc_scores)\n\n# Set graph style\nsns.set(font_scale = 1.75)\nsns.set_style({\"axes.facecolor\": \"1.0\", \"axes.edgecolor\": \"0.85\", \"grid.color\": \"0.85\",\n               \"grid.linestyle\": \"-\", 'axes.labelcolor': '0.4', \"xtick.color\": \"0.4\",\n               'ytick.color': '0.4'})\n\n    \n# Colors\ntraining_color = sns.color_palette(\"RdYlBu\", 10)[1]\ntest_color = sns.color_palette(\"RdYlBu\", 10)[-2]\ncolors = [training_color, test_color]\n\n# Set figure size and create barplot\nf, ax = plt.subplots(figsize=(12, 9))\n\nsns.barplot(x=\"AUC\", y=\"Classifier\", hue=\"AUC Type\", palette = colors,\n            data=auc_scores)\n\n# Generate a bolded horizontal line at y = 0\nax.axvline(x = 0, color = 'black', linewidth = 4, alpha = .7)\n\n# Turn frame off\nax.set_frame_on(False)\n\n# Tight layout\nplt.tight_layout()\n\n# Save Figure\nplt.savefig(\"AUC Scores.png\", dpi = 1080)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}